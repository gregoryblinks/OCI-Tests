import io
import json
import csv
import uuid
import oci
import datetime
import os
import logging

# ---------- CSV schema ----------
CSV_HEADERS = [
    # Event envelope
    "eventType","cloudEventsVersion","payloadVersion","source","eventTime","contentType","eventId",
    # Core problem context
    "tenantId","compartmentId","extensionsCompartmentId","compartmentName","region",
    "problemId","problemName","problemDisplayName","problemType","reason",
    # Severity / risk
    "severity","severityRank","riskLevel",
    # Affected resource & target
    "resourceType","resourceName","resourceId","targetId","lifecycleState",
    # Guidance & timing
    "problemDescription","recommendation","firstDetected","lastDetected",
    # Labels / misc
    "labels",
    # (optional blobs; helpful for debugging/evolution)
    "additionalDetailsJson","rawEventJson"
]

RANK_MAP = {
    "CRITICAL": 4, "CRITICALS": 4,
    "MAJOR": 3, "HIGH": 3,
    "MINOR": 2, "MEDIUM": 2,
    "LOW": 1
}

def _rank(sev: str, risk: str) -> int:
    s = (sev or "").upper()
    r = (risk or "").upper()
    return RANK_MAP.get(s, RANK_MAP.get(r, 0))

def flatten_for_csv(ev: dict) -> dict:
    data = ev.get("data", {}) or {}
    addl = data.get("additionalDetails", {}) or {}
    exts = ev.get("extensions", {}) or {}

    severity = (data.get("severity") or "").upper()
    risk = (addl.get("riskLevel") or data.get("riskLevel") or "")
    labels = addl.get("labels", "")

    # Optional raw blobs for troubleshooting / schema evolution
    addl_json = json.dumps(addl, separators=(",",":")) if addl else ""
    raw_event_json = json.dumps(ev, separators=(",",":")) if ev else ""

    return {
        # Envelope
        "eventType":            ev.get("eventType",""),
        "cloudEventsVersion":   ev.get("cloudEventsVersion",""),
        "payloadVersion":       ev.get("eventTypeVersion",""),
        "source":               ev.get("source",""),
        "eventTime":            ev.get("eventTime",""),
        "contentType":          ev.get("contentType",""),
        "eventId":              ev.get("eventID",""),

        # Core problem context
        "tenantId":             addl.get("tenantId",""),
        "compartmentId":        data.get("compartmentId",""),
        "extensionsCompartmentId": exts.get("compartmentId",""),
        "compartmentName":      data.get("compartmentName",""),
        "region":               addl.get("region",""),

        # Problem identity
        "problemId":            data.get("resourceId",""),   # Cloud Guard problem OCID
        "problemName":          addl.get("problemName") or data.get("resourceName",""),
        "problemDisplayName":   data.get("resourceName",""), # human-friendly display
        "problemType":          addl.get("problemType",""),
        "reason":               addl.get("reason",""),

        # Severity / risk
        "severity":             severity,
        "severityRank":         _rank(severity, risk),
        "riskLevel":            risk,

        # Affected resource & target
        "resourceType":         addl.get("resourceType",""),
        "resourceName":         addl.get("resourceName",""),
        "resourceId":           addl.get("resourceId",""),
        "targetId":             addl.get("targetId",""),
        "lifecycleState":       addl.get("status",""),

        # Guidance & timing
        "problemDescription":   addl.get("problemDescription",""),
        "recommendation":       addl.get("problemRecommendation",""),
        "firstDetected":        addl.get("firstDetected",""),
        "lastDetected":         addl.get("lastDetected",""),

        # Labels / misc
        "labels":               labels,

        # Optional blobs (handy for debugging)
        "additionalDetailsJson": addl_json,
        "rawEventJson":          raw_event_json,
    }
}

def write_csv_to_bucket(object_storage, namespace, bucket_name, rows: list) -> str:
    """
    Writes the given rows (list of dicts) as a single CSV file with header.
    Returns the object key used.
    """
    now = datetime.datetime.utcnow()
    key = f"dt={now:%Y-%m-%d}/hr={now:%H}/findings_{now:%Y%m%d_%H%M%S}_{uuid.uuid4().hex[:8]}.csv"

    buf = io.StringIO()
    writer = csv.DictWriter(buf, fieldnames=CSV_HEADERS, lineterminator="\n", extrasaction="ignore")
    writer.writeheader()
    for r in rows:
        writer.writerow(r)

    body = buf.getvalue().encode("utf-8")

    object_storage.put_object(
        namespace_name=namespace,
        bucket_name=bucket_name,
        object_name=key,
        put_object_body=body,
        content_type="text/csv; charset=utf-8"
    )
    return key

def handler(ctx, data: io.BytesIO = None):
    try:
        # ---- Parse incoming payload (single event or list) ----
        raw_body = data.getvalue().decode("utf-8") if data else "{}"
        payload = json.loads(raw_body) if raw_body else {}
        events = payload if isinstance(payload, list) else [payload]
        logging.getLogger().info("Received %d event(s)", len(events))

        # ---- Ensure env vars ----
        bucket_name = os.getenv("BUCKET_NAME")
        if not bucket_name:
            raise ValueError("BUCKET_NAME environment variable not set")

        compartment_id = os.getenv("COMPARTMENT_OCID")  # Compartment where the bucket lives
        if not compartment_id:
            raise ValueError("COMPARTMENT_OCID environment variable not set")

        # ---- OCI client (Resource Principal) ----
        signer = oci.auth.signers.get_resource_principals_signer()
        object_storage = oci.object_storage.ObjectStorageClient(config={}, signer=signer)
        namespace = object_storage.get_namespace().data
        logging.getLogger().info("Resolved Namespace: %s", namespace)

        # ---- Check bucket exists in that compartment (as you already do) ----
        buckets = object_storage.list_buckets(namespace, compartment_id).data
        bucket_names = [b.name for b in buckets]
        logging.getLogger().info("Buckets in compartment: %s", bucket_names)

        if bucket_name not in bucket_names:
            msg = f"Bucket {bucket_name} NOT found in compartment {compartment_id}"
            logging.getLogger().error(msg)
            return msg

        # ---- NEW: Flatten to CSV rows & write CSV ----
        rows = [flatten_for_csv(ev) for ev in events]
        obj_key = write_csv_to_bucket(object_storage, namespace, bucket_name, rows)

        msg = f"Wrote {len(rows)} row(s) to {bucket_name}/{obj_key}"
        logging.getLogger().info(msg)
        return msg

    except Exception as e:
        logging.getLogger().error("Error processing event: %s", str(e))
        return f"Error: {str(e)}"
